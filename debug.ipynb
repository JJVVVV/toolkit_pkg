{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.zeros((3,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': [[1, 2, 3], [3, 4, 5]]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'a': [[1,2,3], [3,4,5]]}\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': [[1, 2], [3, 4, 5]]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['a'][0] = [1,2]\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "my_dict = {'a': 1, 'b': 2, 'c': 3}\n",
    "value = next(iter(my_dict.values()))\n",
    "print(value)  # 输出 1 或 2 或 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [3, 32, 15, 7]\n",
    "num_tokens_to_remove = 20\n",
    "while num_tokens_to_remove:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9324193000793457\n",
      "[264, 265]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "num = 50000\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for _ in range(num):\n",
    "    lengths = [312, 267]\n",
    "    num_tokens_to_remove = 50\n",
    "    lengths_waiting_to_trunc = [lengths[i] for i in [0,1]]\n",
    "    for _ in range(num_tokens_to_remove):\n",
    "        index_max = lengths_waiting_to_trunc.index(max(lengths_waiting_to_trunc))\n",
    "        lengths_waiting_to_trunc[index_max] -= 1\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)\n",
    "print(lengths_waiting_to_trunc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.97226357460022\n",
      "[310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 311, 267, 311, 267, 311, 267, 311, 267, 311, 267, 311, 267, 311, 267, 311, 267, 311, 267, 311, 267]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "def argmax(l: list):\n",
    "    max_value_idx = 0\n",
    "    max_value = l[0]\n",
    "    idx = 1\n",
    "    while idx<len(l):\n",
    "        if max_value<l[idx]:\n",
    "            max_value, max_value_idx = l[idx], idx\n",
    "        idx += 1\n",
    "    return max_value_idx\n",
    "\n",
    "\n",
    "lengths = [312, 267]\n",
    "x = 50\n",
    "start = time.time()\n",
    "for _ in range(num):\n",
    "    lengths = [312, 267]*30\n",
    "    for _ in range(x):\n",
    "        max_index = argmax(lengths)\n",
    "        lengths[max_index] -= 1\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)\n",
    "print(lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.818470001220703\n",
      "[295 267 295 267 296 267]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "lengths = [312, 267]\n",
    "x = 50\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(num):\n",
    "    lengths = [312, 267]*3\n",
    "    for _ in range(x):\n",
    "        lengths = np.array(lengths)\n",
    "        max_index = np.argmax(lengths)\n",
    "        lengths[max_index] -= 1\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)\n",
    "print(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 2, 3, 6, 7, 8], 'ttt': [5, 4, 3, 6, 7, 8]}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def __truncate(tokenized_dict: dict[str, list[list]], waiting_to_trunc_idxs: list, num_tokens_to_remove: int) -> None:\n",
    "    lengths = [len(value_part) for value_part in tokenized_dict[\"input_ids\"]]\n",
    "    lengths_waiting_to_trunc = [lengths[i] for i in waiting_to_trunc_idxs]\n",
    "    for _ in range(num_tokens_to_remove):\n",
    "        index_max = lengths_waiting_to_trunc.index(max(lengths_waiting_to_trunc))\n",
    "        lengths_waiting_to_trunc[index_max] -= 1\n",
    "    print(lengths_waiting_to_trunc)\n",
    "    for i, j in zip(waiting_to_trunc_idxs, range(len(lengths_waiting_to_trunc))):\n",
    "        lengths[i] = lengths_waiting_to_trunc[j]\n",
    "    for value in tokenized_dict.values():\n",
    "        for i in waiting_to_trunc_idxs:\n",
    "            value[i] = value[i][: lengths[i]]\n",
    "\n",
    "cur_dict = {'input_ids':[[1,2,3,4,5], [6,7,8,9,0]], 'ttt': [[5,4,3,2,1], [6,7,8,9,0]]}\n",
    "__truncate(cur_dict, [0,1], 4)\n",
    "cur_dict\n",
    "cur_dict = {key: sum(value, []) for key, value in cur_dict.items()}\n",
    "cur_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [7592, 1010, 2129, 2024, 2017, 1012, 5292, 3270, 23644, 2050], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer('hello, how are you. hahahha', add_special_tokens=False)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = dict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [7592, 1010, 2129, 2024, 2017, 1012, 5292, 3270, 23644, 2050],\n",
       " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inputs)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.pad(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-26 10:46:29,667 - \u001b[33m WARNING > toolkit.nlp.dataset: model input include 'token_type_ids'. There is a bug causing all the token_type_ids to be zeros\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8bf2e9ceb414de99d87d3ccf5c7e536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenize input texts:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from toolkit.nlp.dataset import TextDataset\n",
    "from transformers import AutoTokenizer\n",
    "import jsonlines\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "def myfun(file_path, tokenizer, model_type, **kargs):\n",
    "    special_tokens_map = tokenizer.special_tokens_map\n",
    "    BOS = special_tokens_map[\"bos_token\"] if \"bos_token\" in special_tokens_map.keys() else None\n",
    "    EOS = special_tokens_map[\"eos_token\"] if \"eos_token\" in special_tokens_map.keys() else None\n",
    "    SEP = special_tokens_map[\"sep_token\"] if \"sep_token\" in special_tokens_map.keys() else None\n",
    "    MASK = special_tokens_map[\"mask_token\"] if \"mask_token\" in special_tokens_map.keys() else None\n",
    "    CLS = special_tokens_map[\"cls_token\"] if \"cls_token\" in special_tokens_map.keys() else None\n",
    "    sep_num = 1\n",
    "\n",
    "    with jsonlines.open(file_path, \"r\") as jlReader:\n",
    "        dict_objs = list(jlReader)\n",
    "        if isinstance(dict_objs[0], str):\n",
    "            dict_objs = dict_objs[1:]\n",
    "\n",
    "    input_texts = []\n",
    "    labels = []\n",
    "    for dict_obj in dict_objs:\n",
    "        # input_texts.append(f\"{BOS}{dict_obj['question1']}{SEP*sep_num}{dict_obj['question2']}{EOS}\")\n",
    "        input_texts.append(((False, CLS), (True, dict_obj[\"question1\"]), (False, SEP), (True, dict_obj[\"question2\"]), (False, SEP)))\n",
    "        labels.append([dict_obj[\"label\"]])\n",
    "        # labels.append(((False, CLS), (True, dict_obj[\"question1\"])))\n",
    "        # labels.append(dict_obj[\"question1\"])\n",
    "    return input_texts, labels\n",
    "\n",
    "\n",
    "dataset = TextDataset(\n",
    "    \"test/data.jsonl\",\n",
    "    \"bert-base-uncased\",\n",
    "    tokenizer,\n",
    "    input_text_format_func=myfun,\n",
    "    padding_side=\"left\",\n",
    "    max_length_input=256,\n",
    "    max_length_label=256,\n",
    "    is_train=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'token_type_ids', 'attention_mask']"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.model_input_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_input': {'input_ids': tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "         101, 2339, 2024, 3060, 1011, 4841, 2061, 3376, 1029,  102, 2339, 2024,\n",
      "        6696, 2015, 2061, 3376, 1029,  102]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1])}, 'first_not_pad_index_input': tensor(36), 'labels': tensor([0], dtype=torch.int32)}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "1\n",
      "[CLS] i want to pursue phd in computer science about social network, what is the open problem in social networks? [SEP] i handle social media for a non - profit. should i start going to social media networking events? are there any good ones in the bay area? [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(dataset.max_length_inputs)\n",
    "print(dataset.max_length_labels)\n",
    "print(tokenizer.decode(dataset[1]['model_input']['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 54])\n",
      "torch.Size([4, 39])\n",
      "torch.Size([4, 46])\n",
      "torch.Size([4, 33])\n",
      "torch.Size([4, 36])\n",
      "torch.Size([4, 48])\n",
      "torch.Size([4, 35])\n",
      "torch.Size([4, 50])\n",
      "{'labels': tensor([[0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0]], dtype=torch.int32), 'input_ids': tensor([[    0,     0,     0,     0,     0,     0,     0,   101,  2054,  2024,\n",
      "          1996,  5918,  2000,  2468,  2343,  1999,  1996,  2142,  2163,  1998,\n",
      "          2129,  2024,  1996,  5918,  2367,  1999, 11959,  1029,   102,  2054,\n",
      "          2024,  1996,  5918,  2000,  2468,  2343,  1999,  1996,  2142,  2163,\n",
      "          1998,  2129,  2024,  1996,  5918,  2367,  1999,  2605,  1029,   102],\n",
      "        [  101,  1000,  2040,  2003,  1996,  4205,  1996,  4581,  2000, 12373,\n",
      "          1011, 17691,  1005,  1055,  1000,  1000,  4205,  1005,  1055,  2299,\n",
      "          1000,  1000,  2024,  2517,  2055,  1029,  1000,   102,  1000,  2029,\n",
      "         12373,  1011, 17691,  2316,  2266,  2626,  1996,  4581,  2000,  1000,\n",
      "          1000,  4205,  1005,  1055,  2299,  1000,  1000,  1029,  1000,   102],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,   101,  2054,  2808,  2323,  1045,\n",
      "          3191,  2004,  2019, 22344, 10670,  1029,   102,  2054,  2024,  1996,\n",
      "          2327,  2808,  2019, 22344,  9458, 10670,  2323,  3191,  1029,   102],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,   101,  2339,  2515,  1037,\n",
      "         10098, 18669, 20553,  2102,  2191,  1037,  2204,  9004,  1029,   102,\n",
      "          2064, 18669, 20553,  3215,  2022,  2204, 18551,  1029,  2054,  2024,\n",
      "          2070,  3971,  2000,  2202,  2204,  2729,  1997,  2068,  1029,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, default_collate\n",
    "\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=4, shuffle=False, \n",
    "                        collate_fn=dataset.collate_fn_padding_right if dataset.padding_side=='right' else dataset.collate_fn_padding_left, \n",
    "                        pin_memory=True)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch['input_ids'].shape)\n",
    "    # print(batch['labels'].shape)\n",
    "    # print(batch['input_ids'].is_pinned())\n",
    "    # print(batch)\n",
    "    # break\n",
    "\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': tensor([[1, 2, 3],\n",
       "         [1, 2, 3]]),\n",
       " 'b': tensor([[4, 5, 6],\n",
       "         [4, 5, 6]]),\n",
       " 'c': ['hh', 'hh']}"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, default_collate\n",
    "\n",
    "default_collate([{'a': torch.tensor([1,2,3]), 'b': torch.tensor([4,5,6]), 'c':'h' 'h'}, {'a': torch.tensor([1,2,3]), 'b': torch.tensor([4,5,6]), 'c':'h' 'h'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.__dict__: {'model_type': '', 'model_name': '', '_name_or_path': '', 'return_dict': True}\n",
      "{'model_type': '', 'model_name': '', '_name_or_path': '', 'return_dict': True}\n",
      "{'return_dict': True}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from toolkit import Config\n",
    "# Config.attribute_alias_map = {'return_dict': 'retrundict'}\n",
    "t = Config(return_dict=True)\n",
    "\n",
    "print(f\"self.__dict__: {t.__dict__}\")\n",
    "print(t.to_dict())\n",
    "print(t.to_diff_dict())\n",
    "t.return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-30 07:33:26,312 - \u001b[32m INFO > toolkit.configuration: Configuration saved in config/train_config.json\u001b[0m\n",
      "bert-base-uncase\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainConfig {\n",
       "  \"a_unknow\": \"aaa\",\n",
       "  \"accumulate_step\": 1,\n",
       "  \"adam_epsilon\": 1e-08,\n",
       "  \"batch_size\": 32,\n",
       "  \"dataset\": \"QQP\",\n",
       "  \"early_stop\": false,\n",
       "  \"early_stop_metric\": \"acc\",\n",
       "  \"epochs\": 5,\n",
       "  \"fp16\": false,\n",
       "  \"learning_rate\": 3e-05,\n",
       "  \"max_length_input\": null,\n",
       "  \"max_length_label\": null,\n",
       "  \"model_name\": \"baseline\",\n",
       "  \"model_type\": \"bert-base-uncase\",\n",
       "  \"problem_type\": null,\n",
       "  \"seed\": 0,\n",
       "  \"test_in_epoch\": false,\n",
       "  \"warmup\": false,\n",
       "  \"warmup_ratio\": -1,\n",
       "  \"weight_decay\": 0.01\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from toolkit.nlp import TrainConfig\n",
    "\n",
    "kwargs = {\n",
    "    \"dataset\":'QQP',\n",
    "    \"early_stop_metric\": 'acc',\n",
    "    \"model_type\": \"bert-base-uncase\",\n",
    "    \"model_name\": \"baseline\",\n",
    "    \"epochs\": 5,\n",
    "    \"batch_size\": 32,\n",
    "    \"learning_rate\": 3e-5,\n",
    "    \"a_unknow\": 'aaa'\n",
    "}\n",
    "tt = TrainConfig(**kwargs)\n",
    "tt.save_pretrained('config')\n",
    "print(tt.model_type)\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-30 07:33:30,424 - \u001b[32m INFO > toolkit.configuration: loading configuration file config/train_config.json\u001b[0m\n",
      "True\n",
      "TrainConfig {\n",
      "  \"a_unknow\": \"aaa\",\n",
      "  \"accumulate_step\": 1,\n",
      "  \"adam_epsilon\": 1e-08,\n",
      "  \"batch_size\": 32,\n",
      "  \"dataset\": \"QQP\",\n",
      "  \"early_stop\": false,\n",
      "  \"early_stop_metric\": \"acc\",\n",
      "  \"epochs\": 5,\n",
      "  \"fp16\": false,\n",
      "  \"learning_rate\": 3e-05,\n",
      "  \"max_length_input\": null,\n",
      "  \"max_length_label\": null,\n",
      "  \"model_name\": \"baseline\",\n",
      "  \"model_type\": \"bert-base-uncase\",\n",
      "  \"problem_type\": null,\n",
      "  \"seed\": 0,\n",
      "  \"test_in_epoch\": false,\n",
      "  \"warmup\": false,\n",
      "  \"warmup_ratio\": -1,\n",
      "  \"weight_decay\": 0.01\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt_load = TrainConfig.from_pretrained('train_config.json')\n",
    "print(tt==tt_load)\n",
    "print(tt_load)\n",
    "tt.max_length_input is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert\n",
      "{'_name_or_path': '', 'train_return_dict': False, 'ttt': 2, 'model_type': 'bert'}\n",
      "base:        {'_name_or_path': '', 'model_type': ''}\n",
      "class specific: {'_name_or_path': '', 'train_return_dict': False, 'ttt': 1, 'model_type': 'bert'}\n",
      "{'train_return_dict': False, 'ttt': 2, 'model_type': 'bert'}\n"
     ]
    }
   ],
   "source": [
    "class TrainConfig(Config):\n",
    "    model_type='bert'\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.train_return_dict = False\n",
    "        self.ttt = 1\n",
    "\n",
    "st = TrainConfig()\n",
    "print(st.model_type)\n",
    "st.ttt=2\n",
    "print(st.to_dict())\n",
    "print(st.to_diff_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-29 14:21:40,606 - \u001b[33m WARNING > toolkit.configuration: bert. This is not supported for all configurations of models and can yield errors.\u001b[0m\n",
      "2023-06-29 14:21:40,606 - \u001b[33m WARNING > toolkit.configuration: bert. This is not supported for all configurations of models and can yield errors.\u001b[0m\n",
      "2023-06-29 14:21:40,606 - \u001b[33m WARNING > toolkit.configuration: bert. This is not supported for all configurations of models and can yield errors.\u001b[0m\n",
      "2023-06-29 14:21:40,606 - \u001b[33m WARNING > toolkit.configuration: bert. This is not supported for all configurations of models and can yield errors.\u001b[0m\n",
      "2023-06-29 14:21:40,606 - \u001b[33m WARNING > toolkit.configuration: bert. This is not supported for all configurations of models and can yield errors.\u001b[0m\n",
      "2023-06-29 14:21:40,606 - \u001b[33m WARNING > toolkit.configuration: bert. This is not supported for all configurations of models and can yield errors.\u001b[0m\n",
      "2023-06-29 14:21:40,606 - \u001b[33m WARNING > toolkit.configuration: bert. This is not supported for all configurations of models and can yield errors.\u001b[0m\n",
      "2023-06-29 14:21:40,606 - \u001b[33m WARNING > toolkit.configuration: bert. This is not supported for all configurations of models and can yield errors.\u001b[0m\n",
      "2023-06-29 14:21:40,606 - \u001b[33m WARNING > toolkit.configuration: bert. This is not supported for all configurations of models and can yield errors.\u001b[0m\n",
      "2023-06-29 14:21:40,606 - \u001b[33m WARNING > toolkit.configuration: bert. This is not supported for all configurations of models and can yield errors.\u001b[0m\n",
      "2023-06-29 14:21:40,606 - \u001b[33m WARNING > toolkit.configuration: bert. This is not supported for all configurations of models and can yield errors.\u001b[0m\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m TrainConfig\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/lib/toolkit/toolkit/configuration.py:81\u001b[0m, in \u001b[0;36mConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     76\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained\u001b[39m(\u001b[39mcls\u001b[39m, pretrained_model_name_or_path: Path \u001b[39m|\u001b[39m \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mConfig\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     77\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m     78\u001b[0m         \u001b[39m# f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\u001b[39;00m\n\u001b[1;32m     79\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodel_type\u001b[39m}\u001b[39;00m\u001b[39m. This is not supported for all configurations of models and can yield errors.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     80\u001b[0m     )\n\u001b[0;32m---> 81\u001b[0m     config_dict \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     82\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodel_type:\n\u001b[1;32m     83\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m     84\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou are using a model of type \u001b[39m\u001b[39m{\u001b[39;00mconfig_dict[\u001b[39m'\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m to instantiate a model of type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodel_type\u001b[39m}\u001b[39;00m\u001b[39m. This is not supported for all configurations of models and can yield errors.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     86\u001b[0m         )\n",
      "File \u001b[0;32m~/lib/toolkit/toolkit/configuration.py:106\u001b[0m, in \u001b[0;36mConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    105\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m config_dict \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    108\u001b[0m \u001b[39m# That config file may point us toward another config file to use.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mconfiguration_files\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[1;32m    110\u001b[0m     \u001b[39m# The another config file must be a path or be in the same folder as the first\u001b[39;00m\n",
      "File \u001b[0;32m~/lib/toolkit/toolkit/configuration.py:129\u001b[0m, in \u001b[0;36mConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m# Load config dict\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     config_dict \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_dict_from_json_file(resolved_config_file)\n\u001b[1;32m    130\u001b[0m \u001b[39mexcept\u001b[39;00m (json\u001b[39m.\u001b[39mJSONDecodeError, \u001b[39mUnicodeDecodeError\u001b[39;00m):\n\u001b[1;32m    131\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIt looks like the config file at \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mresolved_config_file\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is not a valid JSON file.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/lib/toolkit/toolkit/configuration.py:149\u001b[0m, in \u001b[0;36mConfig._dict_from_json_file\u001b[0;34m(json_file)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_dict_from_json_file\u001b[39m(json_file: Path \u001b[39m|\u001b[39m \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict:\n\u001b[0;32m--> 149\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(json_file, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m reader:\n\u001b[1;32m    150\u001b[0m         text \u001b[39m=\u001b[39m reader\u001b[39m.\u001b[39mread()\n\u001b[1;32m    151\u001b[0m     \u001b[39mreturn\u001b[39;00m json\u001b[39m.\u001b[39mloads(text)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'config'"
     ]
    }
   ],
   "source": [
    "TrainConfig.from_pretrained('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attribute_alias_map\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'return_dict': 'retrundict'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.attribute_alias_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "return_dict\n",
      "retrundict\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrundict\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.retrundict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jjwang/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "class Path(os.PathLike):\n",
    "    def __init__(self, path):\n",
    "        self.__path = path\n",
    "\n",
    "    def __fspath__(self):\n",
    "        return self.__path.replace(\"/\", os.path.sep)\n",
    "\n",
    "    \n",
    "path = Path('/home/jjwang/')\n",
    "print(path.__fspath__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path('./test/data.jsonl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "复杂推理是指理解和利用支持证据或逻辑得出结论或做出决定的能力[51,52]。\n",
      "根据推理过程中涉及的逻辑和证据的类型，将现有的评估任务分为知识推理、符号推理和数学推理3大类。\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"\"\"\n",
    "复杂推理是指理解和利用支持证据或逻辑得出结论或做出决定的能力[51,52]。根据推理过程中涉及的逻辑和证据的类型，将现有的评估任务分为知识推理、符号推理和数学推理3大类。\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "text = re.sub(r'。', '。\\n', text)\n",
    "text = re.sub(r'\\(', '（', text)\n",
    "text = re.sub(r'\\)', '）', text)\n",
    "text = re.sub(r'llm', 'LLM', text)\n",
    "text = re.sub(r'plm', 'PLM', text)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "p = Path('setup.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.is_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.is_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('config/setup.cfg')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'config'/p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
