{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.zeros((3,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': [[1, 2, 3], [3, 4, 5]]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'a': [[1,2,3], [3,4,5]]}\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': [[1, 2], [3, 4, 5]]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['a'][0] = [1,2]\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "my_dict = {'a': 1, 'b': 2, 'c': 3}\n",
    "value = next(iter(my_dict.values()))\n",
    "print(value)  # 输出 1 或 2 或 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [3, 32, 15, 7]\n",
    "num_tokens_to_remove = 20\n",
    "while num_tokens_to_remove:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9324193000793457\n",
      "[264, 265]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "num = 50000\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for _ in range(num):\n",
    "    lengths = [312, 267]\n",
    "    num_tokens_to_remove = 50\n",
    "    lengths_waiting_to_trunc = [lengths[i] for i in [0,1]]\n",
    "    for _ in range(num_tokens_to_remove):\n",
    "        index_max = lengths_waiting_to_trunc.index(max(lengths_waiting_to_trunc))\n",
    "        lengths_waiting_to_trunc[index_max] -= 1\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)\n",
    "print(lengths_waiting_to_trunc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.97226357460022\n",
      "[310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 311, 267, 311, 267, 311, 267, 311, 267, 311, 267, 311, 267, 311, 267, 311, 267, 311, 267, 311, 267]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "def argmax(l: list):\n",
    "    max_value_idx = 0\n",
    "    max_value = l[0]\n",
    "    idx = 1\n",
    "    while idx<len(l):\n",
    "        if max_value<l[idx]:\n",
    "            max_value, max_value_idx = l[idx], idx\n",
    "        idx += 1\n",
    "    return max_value_idx\n",
    "\n",
    "\n",
    "lengths = [312, 267]\n",
    "x = 50\n",
    "start = time.time()\n",
    "for _ in range(num):\n",
    "    lengths = [312, 267]*30\n",
    "    for _ in range(x):\n",
    "        max_index = argmax(lengths)\n",
    "        lengths[max_index] -= 1\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)\n",
    "print(lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.818470001220703\n",
      "[295 267 295 267 296 267]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "lengths = [312, 267]\n",
    "x = 50\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(num):\n",
    "    lengths = [312, 267]*3\n",
    "    for _ in range(x):\n",
    "        lengths = np.array(lengths)\n",
    "        max_index = np.argmax(lengths)\n",
    "        lengths[max_index] -= 1\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)\n",
    "print(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 2, 3, 6, 7, 8], 'ttt': [5, 4, 3, 6, 7, 8]}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def __truncate(tokenized_dict: dict[str, list[list]], waiting_to_trunc_idxs: list, num_tokens_to_remove: int) -> None:\n",
    "    lengths = [len(value_part) for value_part in tokenized_dict[\"input_ids\"]]\n",
    "    lengths_waiting_to_trunc = [lengths[i] for i in waiting_to_trunc_idxs]\n",
    "    for _ in range(num_tokens_to_remove):\n",
    "        index_max = lengths_waiting_to_trunc.index(max(lengths_waiting_to_trunc))\n",
    "        lengths_waiting_to_trunc[index_max] -= 1\n",
    "    print(lengths_waiting_to_trunc)\n",
    "    for i, j in zip(waiting_to_trunc_idxs, range(len(lengths_waiting_to_trunc))):\n",
    "        lengths[i] = lengths_waiting_to_trunc[j]\n",
    "    for value in tokenized_dict.values():\n",
    "        for i in waiting_to_trunc_idxs:\n",
    "            value[i] = value[i][: lengths[i]]\n",
    "\n",
    "cur_dict = {'input_ids':[[1,2,3,4,5], [6,7,8,9,0]], 'ttt': [[5,4,3,2,1], [6,7,8,9,0]]}\n",
    "__truncate(cur_dict, [0,1], 4)\n",
    "cur_dict\n",
    "cur_dict = {key: sum(value, []) for key, value in cur_dict.items()}\n",
    "cur_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [7592, 1010, 2129, 2024, 2017, 1012, 5292, 3270, 23644, 2050], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer('hello, how are you. hahahha', add_special_tokens=False)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = dict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [7592, 1010, 2129, 2024, 2017, 1012, 5292, 3270, 23644, 2050],\n",
       " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inputs)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.pad(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-26 10:46:29,667 - \u001b[33m WARNING > toolkit.nlp.dataset: model input include 'token_type_ids'. There is a bug causing all the token_type_ids to be zeros\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8bf2e9ceb414de99d87d3ccf5c7e536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenize input texts:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from toolkit.nlp.dataset import TextDataset\n",
    "from transformers import AutoTokenizer\n",
    "import jsonlines\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "def myfun(file_path, tokenizer, model_type, **kargs):\n",
    "    special_tokens_map = tokenizer.special_tokens_map\n",
    "    BOS = special_tokens_map[\"bos_token\"] if \"bos_token\" in special_tokens_map.keys() else None\n",
    "    EOS = special_tokens_map[\"eos_token\"] if \"eos_token\" in special_tokens_map.keys() else None\n",
    "    SEP = special_tokens_map[\"sep_token\"] if \"sep_token\" in special_tokens_map.keys() else None\n",
    "    MASK = special_tokens_map[\"mask_token\"] if \"mask_token\" in special_tokens_map.keys() else None\n",
    "    CLS = special_tokens_map[\"cls_token\"] if \"cls_token\" in special_tokens_map.keys() else None\n",
    "    sep_num = 1\n",
    "\n",
    "    with jsonlines.open(file_path, \"r\") as jlReader:\n",
    "        dict_objs = list(jlReader)\n",
    "        if isinstance(dict_objs[0], str):\n",
    "            dict_objs = dict_objs[1:]\n",
    "\n",
    "    input_texts = []\n",
    "    labels = []\n",
    "    for dict_obj in dict_objs:\n",
    "        # input_texts.append(f\"{BOS}{dict_obj['question1']}{SEP*sep_num}{dict_obj['question2']}{EOS}\")\n",
    "        input_texts.append(((False, CLS), (True, dict_obj[\"question1\"]), (False, SEP), (True, dict_obj[\"question2\"]), (False, SEP)))\n",
    "        labels.append([dict_obj[\"label\"]])\n",
    "        # labels.append(((False, CLS), (True, dict_obj[\"question1\"])))\n",
    "        # labels.append(dict_obj[\"question1\"])\n",
    "    return input_texts, labels\n",
    "\n",
    "\n",
    "dataset = TextDataset(\n",
    "    \"test/data.jsonl\",\n",
    "    \"bert-base-uncased\",\n",
    "    tokenizer,\n",
    "    input_text_format_func=myfun,\n",
    "    padding_side=\"left\",\n",
    "    max_length_input=256,\n",
    "    max_length_label=256,\n",
    "    is_train=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'token_type_ids', 'attention_mask']"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.model_input_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_input': {'input_ids': tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "         101, 2339, 2024, 3060, 1011, 4841, 2061, 3376, 1029,  102, 2339, 2024,\n",
      "        6696, 2015, 2061, 3376, 1029,  102]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1])}, 'first_not_pad_index_input': tensor(36), 'labels': tensor([0], dtype=torch.int32)}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "1\n",
      "[CLS] i want to pursue phd in computer science about social network, what is the open problem in social networks? [SEP] i handle social media for a non - profit. should i start going to social media networking events? are there any good ones in the bay area? [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(dataset.max_length_inputs)\n",
    "print(dataset.max_length_labels)\n",
    "print(tokenizer.decode(dataset[1]['model_input']['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 54])\n",
      "torch.Size([4, 39])\n",
      "torch.Size([4, 46])\n",
      "torch.Size([4, 33])\n",
      "torch.Size([4, 36])\n",
      "torch.Size([4, 48])\n",
      "torch.Size([4, 35])\n",
      "torch.Size([4, 50])\n",
      "{'labels': tensor([[0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0]], dtype=torch.int32), 'input_ids': tensor([[    0,     0,     0,     0,     0,     0,     0,   101,  2054,  2024,\n",
      "          1996,  5918,  2000,  2468,  2343,  1999,  1996,  2142,  2163,  1998,\n",
      "          2129,  2024,  1996,  5918,  2367,  1999, 11959,  1029,   102,  2054,\n",
      "          2024,  1996,  5918,  2000,  2468,  2343,  1999,  1996,  2142,  2163,\n",
      "          1998,  2129,  2024,  1996,  5918,  2367,  1999,  2605,  1029,   102],\n",
      "        [  101,  1000,  2040,  2003,  1996,  4205,  1996,  4581,  2000, 12373,\n",
      "          1011, 17691,  1005,  1055,  1000,  1000,  4205,  1005,  1055,  2299,\n",
      "          1000,  1000,  2024,  2517,  2055,  1029,  1000,   102,  1000,  2029,\n",
      "         12373,  1011, 17691,  2316,  2266,  2626,  1996,  4581,  2000,  1000,\n",
      "          1000,  4205,  1005,  1055,  2299,  1000,  1000,  1029,  1000,   102],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,   101,  2054,  2808,  2323,  1045,\n",
      "          3191,  2004,  2019, 22344, 10670,  1029,   102,  2054,  2024,  1996,\n",
      "          2327,  2808,  2019, 22344,  9458, 10670,  2323,  3191,  1029,   102],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,   101,  2339,  2515,  1037,\n",
      "         10098, 18669, 20553,  2102,  2191,  1037,  2204,  9004,  1029,   102,\n",
      "          2064, 18669, 20553,  3215,  2022,  2204, 18551,  1029,  2054,  2024,\n",
      "          2070,  3971,  2000,  2202,  2204,  2729,  1997,  2068,  1029,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, default_collate\n",
    "\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=4, shuffle=False, \n",
    "                        collate_fn=dataset.collate_fn_padding_right if dataset.padding_side=='right' else dataset.collate_fn_padding_left, \n",
    "                        pin_memory=True)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch['input_ids'].shape)\n",
    "    # print(batch['labels'].shape)\n",
    "    # print(batch['input_ids'].is_pinned())\n",
    "    # print(batch)\n",
    "    # break\n",
    "\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': tensor([[1, 2, 3],\n",
       "         [1, 2, 3]]),\n",
       " 'b': tensor([[4, 5, 6],\n",
       "         [4, 5, 6]]),\n",
       " 'c': ['hh', 'hh']}"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, default_collate\n",
    "\n",
    "default_collate([{'a': torch.tensor([1,2,3]), 'b': torch.tensor([4,5,6]), 'c':'h' 'h'}, {'a': torch.tensor([1,2,3]), 'b': torch.tensor([4,5,6]), 'c':'h' 'h'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.__dict__: {'model_type': '', 'model_name': '', '_name_or_path': '', 'return_dict': True}\n",
      "{'model_type': '', 'model_name': '', '_name_or_path': '', 'return_dict': True}\n",
      "{'return_dict': True}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from toolkit import Config\n",
    "# Config.attribute_alias_map = {'return_dict': 'retrundict'}\n",
    "t = Config(return_dict=True)\n",
    "\n",
    "print(f\"self.__dict__: {t.__dict__}\")\n",
    "print(t.to_dict())\n",
    "print(t.to_diff_dict())\n",
    "t.return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-30 09:21:32,462 - \u001b[32m INFO > toolkit.configuration: Configuration saved in config/train_config.json\u001b[0m\n",
      "32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NLPTrainConfig {\n",
       "  \"a_unknow\": \"aaa\",\n",
       "  \"accumulate_step\": 1,\n",
       "  \"adam_epsilon\": 1e-08,\n",
       "  \"batch_size\": \"baseline\",\n",
       "  \"dataset\": \"QQP\",\n",
       "  \"early_stop\": false,\n",
       "  \"early_stop_metric\": \"acc\",\n",
       "  \"epochs\": \"bert-base-uncase\",\n",
       "  \"fp16\": false,\n",
       "  \"learning_rate\": 5,\n",
       "  \"max_length_input\": 256,\n",
       "  \"max_length_label\": null,\n",
       "  \"model_name\": 3e-05,\n",
       "  \"model_type\": 32,\n",
       "  \"problem_type\": null,\n",
       "  \"seed\": 0,\n",
       "  \"test_in_epoch\": false,\n",
       "  \"warmup\": false,\n",
       "  \"warmup_ratio\": -1,\n",
       "  \"weight_decay\": 0.01\n",
       "}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from toolkit.nlp import NLPTrainConfig\n",
    "\n",
    "kwargs = {\n",
    "    \"dataset\":'QQP',\n",
    "    \"early_stop_metric\": 'acc',\n",
    "    \"model_type\": \"bert-base-uncase\",\n",
    "    \"model_name\": \"baseline\",\n",
    "    \"epochs\": 5,\n",
    "    \"batch_size\": 32,\n",
    "    \"learning_rate\": 3e-5,\n",
    "    \"a_unknow\": 'aaa',\n",
    "    \"max_length_input\":256\n",
    "}\n",
    "tt = NLPTrainConfig(**kwargs)\n",
    "tt.save_pretrained('config')\n",
    "print(tt.model_type)\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-30 09:21:33,879 - \u001b[32m INFO > toolkit.configuration: loading configuration file config/train_config.json\u001b[0m\n",
      "False\n",
      "NLPTrainConfig {\n",
      "  \"a_unknow\": \"aaa\",\n",
      "  \"accumulate_step\": 1,\n",
      "  \"adam_epsilon\": 1e-08,\n",
      "  \"batch_size\": 3e-05,\n",
      "  \"dataset\": \"QQP\",\n",
      "  \"early_stop\": false,\n",
      "  \"early_stop_metric\": \"acc\",\n",
      "  \"epochs\": 32,\n",
      "  \"fp16\": false,\n",
      "  \"learning_rate\": \"bert-base-uncase\",\n",
      "  \"max_length_input\": 256,\n",
      "  \"max_length_label\": null,\n",
      "  \"model_name\": 5,\n",
      "  \"model_type\": \"baseline\",\n",
      "  \"problem_type\": null,\n",
      "  \"seed\": 0,\n",
      "  \"test_in_epoch\": false,\n",
      "  \"warmup\": false,\n",
      "  \"warmup_ratio\": -1,\n",
      "  \"weight_decay\": 0.01\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt_load = NLPTrainConfig.from_pretrained('train_config.json')\n",
    "print(tt==tt_load)\n",
    "print(tt_load)\n",
    "tt.max_length_input is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert\n",
      "{'_name_or_path': '', 'train_return_dict': False, 'ttt': 2, 'model_type': 'bert'}\n",
      "base:        {'_name_or_path': '', 'model_type': ''}\n",
      "class specific: {'_name_or_path': '', 'train_return_dict': False, 'ttt': 1, 'model_type': 'bert'}\n",
      "{'train_return_dict': False, 'ttt': 2, 'model_type': 'bert'}\n"
     ]
    }
   ],
   "source": [
    "class TrainConfig(Config):\n",
    "    model_type='bert'\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.train_return_dict = False\n",
    "        self.ttt = 1\n",
    "\n",
    "st = TrainConfig()\n",
    "print(st.model_type)\n",
    "st.ttt=2\n",
    "print(st.to_dict())\n",
    "print(st.to_diff_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-29 14:21:40,606 - \u001b[33m WARNING > toolkit.configuration: bert. This is not supported for all configurations of models and can yield errors.\u001b[0m\n",
      "2023-06-29 14:21:40,606 - \u001b[33m WARNING > toolkit.configuration: bert. This is not supported for all configurations of models and can yield errors.\u001b[0m\n",
      "2023-06-29 14:21:40,606 - \u001b[33m WARNING > toolkit.configuration: bert. This is not supported for all configurations of models and can yield errors.\u001b[0m\n",
      "2023-06-29 14:21:40,606 - \u001b[33m WARNING > toolkit.configuration: bert. This is not supported for all configurations of models and can yield errors.\u001b[0m\n",
      "2023-06-29 14:21:40,606 - \u001b[33m WARNING > toolkit.configuration: bert. This is not supported for all configurations of models and can yield errors.\u001b[0m\n",
      "2023-06-29 14:21:40,606 - \u001b[33m WARNING > toolkit.configuration: bert. This is not supported for all configurations of models and can yield errors.\u001b[0m\n",
      "2023-06-29 14:21:40,606 - \u001b[33m WARNING > toolkit.configuration: bert. This is not supported for all configurations of models and can yield errors.\u001b[0m\n",
      "2023-06-29 14:21:40,606 - \u001b[33m WARNING > toolkit.configuration: bert. This is not supported for all configurations of models and can yield errors.\u001b[0m\n",
      "2023-06-29 14:21:40,606 - \u001b[33m WARNING > toolkit.configuration: bert. This is not supported for all configurations of models and can yield errors.\u001b[0m\n",
      "2023-06-29 14:21:40,606 - \u001b[33m WARNING > toolkit.configuration: bert. This is not supported for all configurations of models and can yield errors.\u001b[0m\n",
      "2023-06-29 14:21:40,606 - \u001b[33m WARNING > toolkit.configuration: bert. This is not supported for all configurations of models and can yield errors.\u001b[0m\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m TrainConfig\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/lib/toolkit/toolkit/configuration.py:81\u001b[0m, in \u001b[0;36mConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     76\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained\u001b[39m(\u001b[39mcls\u001b[39m, pretrained_model_name_or_path: Path \u001b[39m|\u001b[39m \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mConfig\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     77\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m     78\u001b[0m         \u001b[39m# f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\u001b[39;00m\n\u001b[1;32m     79\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodel_type\u001b[39m}\u001b[39;00m\u001b[39m. This is not supported for all configurations of models and can yield errors.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     80\u001b[0m     )\n\u001b[0;32m---> 81\u001b[0m     config_dict \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     82\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodel_type:\n\u001b[1;32m     83\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m     84\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou are using a model of type \u001b[39m\u001b[39m{\u001b[39;00mconfig_dict[\u001b[39m'\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m to instantiate a model of type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodel_type\u001b[39m}\u001b[39;00m\u001b[39m. This is not supported for all configurations of models and can yield errors.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     86\u001b[0m         )\n",
      "File \u001b[0;32m~/lib/toolkit/toolkit/configuration.py:106\u001b[0m, in \u001b[0;36mConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    105\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m config_dict \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    108\u001b[0m \u001b[39m# That config file may point us toward another config file to use.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mconfiguration_files\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[1;32m    110\u001b[0m     \u001b[39m# The another config file must be a path or be in the same folder as the first\u001b[39;00m\n",
      "File \u001b[0;32m~/lib/toolkit/toolkit/configuration.py:129\u001b[0m, in \u001b[0;36mConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m# Load config dict\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     config_dict \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_dict_from_json_file(resolved_config_file)\n\u001b[1;32m    130\u001b[0m \u001b[39mexcept\u001b[39;00m (json\u001b[39m.\u001b[39mJSONDecodeError, \u001b[39mUnicodeDecodeError\u001b[39;00m):\n\u001b[1;32m    131\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIt looks like the config file at \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mresolved_config_file\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is not a valid JSON file.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/lib/toolkit/toolkit/configuration.py:149\u001b[0m, in \u001b[0;36mConfig._dict_from_json_file\u001b[0;34m(json_file)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_dict_from_json_file\u001b[39m(json_file: Path \u001b[39m|\u001b[39m \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict:\n\u001b[0;32m--> 149\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(json_file, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m reader:\n\u001b[1;32m    150\u001b[0m         text \u001b[39m=\u001b[39m reader\u001b[39m.\u001b[39mread()\n\u001b[1;32m    151\u001b[0m     \u001b[39mreturn\u001b[39;00m json\u001b[39m.\u001b[39mloads(text)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'config'"
     ]
    }
   ],
   "source": [
    "TrainConfig.from_pretrained('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attribute_alias_map\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'return_dict': 'retrundict'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.attribute_alias_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "return_dict\n",
      "retrundict\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrundict\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.retrundict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jjwang/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "class Path(os.PathLike):\n",
    "    def __init__(self, path):\n",
    "        self.__path = path\n",
    "\n",
    "    def __fspath__(self):\n",
    "        return self.__path.replace(\"/\", os.path.sep)\n",
    "\n",
    "    \n",
    "path = Path('/home/jjwang/')\n",
    "print(path.__fspath__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path('./test/data.jsonl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "复杂推理是指理解和利用支持证据或逻辑得出结论或做出决定的能力[51,52]。\n",
      "根据推理过程中涉及的逻辑和证据的类型，将现有的评估任务分为知识推理、符号推理和数学推理3大类。\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"\"\"\n",
    "复杂推理是指理解和利用支持证据或逻辑得出结论或做出决定的能力[51,52]。根据推理过程中涉及的逻辑和证据的类型，将现有的评估任务分为知识推理、符号推理和数学推理3大类。\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "text = re.sub(r'。', '。\\n', text)\n",
    "text = re.sub(r'\\(', '（', text)\n",
    "text = re.sub(r'\\)', '）', text)\n",
    "text = re.sub(r'llm', 'LLM', text)\n",
    "text = re.sub(r'plm', 'PLM', text)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "p = Path('setup.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.is_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.is_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('config/setup.cfg')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'config'/p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def check_mem(cuda_device_id: int)->tuple[int,int]:\n",
    "    \"\"\"Get total and used memory (unit: `MB`) of GPU with the corresponding ID.\"\"\"\n",
    "    devices_info = os.popen('\"/usr/bin/nvidia-smi\" --query-gpu=memory.total,memory.used --format=csv,nounits,noheader').read().strip().split(\"\\n\")\n",
    "    total, used = devices_info[int(cuda_device_id)].split(\",\")\n",
    "    return int(total), int(used)\n",
    "\n",
    "check_mem(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 函数设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import math\n",
    "import random\n",
    "random.seed(1)\n",
    "\n",
    "def log_func(x, n):\n",
    "    n = torch.tensor(n)\n",
    "    return torch.log(x)/torch.log(n)\n",
    "\n",
    "\n",
    "# def randomcolor():\n",
    "#     colorArr = [ '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F' ]\n",
    "#     color = \"\"\n",
    "#     for i in range (6):\n",
    "#         color += colorArr[random.randint (0, 14)]\n",
    "#     return \"#\" +color\n",
    "\n",
    "\n",
    "def randomcolor():\n",
    "    colorArr = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F' ]\n",
    "    color = \"\"\n",
    "    for i in range(6):\n",
    "        color += colorArr[random.randint(0, 15)]\n",
    "    return \"#\"+color\n",
    "\n",
    "\n",
    "# plt.rc('font',family='HYZhengYuan')\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['axes.unicode_minus']=False #用来正常显示负号\n",
    "\n",
    "logits = torch.tensor([list(np.linspace(-10, 10, 1000, endpoint=True))], dtype=torch.float).T\n",
    "labels_pos = torch.ones((logits.shape[0], 1), dtype=int)\n",
    "labels_neg = torch.zeros((logits.shape[0], 1), dtype=int)\n",
    "y = torch.sigmoid(logits).detach()\n",
    "\n",
    "\n",
    "\n",
    "# # ----------------------------------------------------------------------------------------------------\n",
    "# alpha = -0.5\n",
    "# beta = 10\n",
    "# weight_loss1 = torch.ones((logits.shape[0], 1), device=logits.device)\n",
    "# already_classable = ((logits>0)==labels).squeeze()\n",
    "# weight_loss1[already_classable] = torch.sigmoid(((abs(labels-y).detach()+alpha)[already_classable])*beta)\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "alpha = 0\n",
    "beta = 2\n",
    "gama = 0.9\n",
    "delta = 1-gama\n",
    "weight_loss2_10 = torch.ones((logits.shape[0], 1), device=logits.device)\n",
    "already_classable = ((logits>0)==labels_pos).squeeze()\n",
    "x = ((abs(labels_pos-y).detach()+alpha)*beta)[already_classable]*gama  # x: [game, 0]\n",
    "# print(x[0], x[-1])\n",
    "x = gama-x+delta\n",
    "# print(x[0], x[-1])\n",
    "# weight_loss2[already_classable] = torch.log(((abs(labels-y).detach()+alpha)[already_classable])*beta)+1\n",
    "weight_loss2_10[already_classable] = -torch.log10(x)\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "alpha = 0\n",
    "beta = 2\n",
    "gama = 0.5\n",
    "delta = 1-gama\n",
    "weight_loss2_2 = torch.ones((logits.shape[0], 1), device=logits.device)\n",
    "already_classable = ((logits>0)==labels_pos).squeeze()\n",
    "x = ((abs(labels_pos-y).detach()+alpha)*beta)[already_classable]*gama  # x: [game, 0]\n",
    "# print(x[0], x[-1])\n",
    "x = gama-x+delta\n",
    "# print(x[0], x[-1])\n",
    "# weight_loss2[already_classable] = torch.log(((abs(labels-y).detach()+alpha)[already_classable])*beta)+1\n",
    "weight_loss2_2[already_classable] = -torch.log2(x)\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# base=100\n",
    "\n",
    "# alpha = 0\n",
    "# beta = 2\n",
    "# gama = 1-1/base\n",
    "# delta = 1-gama\n",
    "# weight_loss2_n = torch.ones((logits.shape[0], 1), device=logits.device)\n",
    "# already_classable = ((logits>0)==labels).squeeze()\n",
    "# x = ((abs(labels-y).detach()+alpha)*beta)[already_classable]*gama  # x: [game, 0]\n",
    "# # print(x[0], x[-1])\n",
    "# x = gama-x+delta\n",
    "# # print(x[0], x[-1])\n",
    "# # weight_loss2[already_classable] = torch.log(((abs(labels-y).detach()+alpha)[already_classable])*beta)+1\n",
    "# weight_loss2_n[already_classable] = -log(x, base)\n",
    "\n",
    "base=1000\n",
    "def get_weights_loss(logits, labels, base):\n",
    "    logits = logits.detach()\n",
    "    labels = labels.detach()\n",
    "    y = torch.sigmoid(logits)\n",
    "    gama = 1-1/base\n",
    "    weight_loss = torch.ones((logits.shape[0], 1), device=logits.device)\n",
    "    already_classable = ((logits>0)==labels.bool()).squeeze()\n",
    "    x = (abs(labels-y)*2)[already_classable]*(gama)  # x: [game, 0]\n",
    "    x = 1-x\n",
    "    weight_loss[already_classable] = -log_func(x, base)\n",
    "    return weight_loss\n",
    "weight_loss2_n = get_weights_loss(logits, labels_pos, base)\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# alpha = 0\n",
    "# beta = 2\n",
    "# gama = 5\n",
    "# weight_loss3 = torch.ones((logits.shape[0], 1), device=logits.device)\n",
    "# already_classable = ((logits>0)==labels).squeeze()\n",
    "# x = ((abs(labels-y).detach()+alpha)*beta)[already_classable]*gama\n",
    "# weight_loss3[already_classable] = -(torch.tanh(gama-x))+1\n",
    "# # ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Increase font size\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "# Use consistent line width and style\n",
    "line_style = {'linewidth': 1.25, 'linestyle': '-'}\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "# plt.rcParams['font.serif'] = ['CMU Serif'] + plt.rcParams['font.serif']\n",
    "plt.rcParams['axes.linewidth'] = 1.5\n",
    "plt.rcParams['xtick.major.width'] = 0.5\n",
    "plt.rcParams['ytick.major.width'] = 0.5\n",
    "plt.rcParams['lines.linewidth'] = 1.25\n",
    "\n",
    "# Use a pastel color scheme\n",
    "# colors = sns.color_palette('husl', 4)\n",
    "colors = sns.color_palette(\"Greens\", 4)\n",
    "# colors = sns.dark_palette(\"Greens\", 4, reverse=True)\n",
    "# colors = sns.dark_palette(\"deep\", 4, reverse=True)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 4), dpi=300)\n",
    "\n",
    "# Plot the positive labels\n",
    "for i, log in enumerate([2, 10, 100, 1000]):\n",
    "    ax1.plot(y, get_weights_loss(logits, labels_pos, log), color=colors[i], label=f'log {log}', **line_style)\n",
    "\n",
    "# # Improve axis labels\n",
    "# ax1.set_xlabel('Probability of prediction')\n",
    "# ax1.set_ylabel('Weight')\n",
    "\n",
    "# # Simplify the legend\n",
    "# ax1.legend(loc='upper right', frameon=False)\n",
    "\n",
    "# Plot the negative labels\n",
    "for i, log in enumerate([2, 10, 100, 1000]):\n",
    "    ax2.plot(y, get_weights_loss(logits, labels_neg, log), color=colors[i], label=f'log {log}', **line_style)\n",
    "\n",
    "# # Improve axis labels\n",
    "# ax2.set_xlabel('Probability of prediction')\n",
    "# ax2.set_ylabel('Weight')\n",
    "\n",
    "# # Simplify the legend\n",
    "# ax2.legend(loc='upper left', frameon=False)\n",
    "\n",
    "# Remove top and right borders\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Increase line width of remaining borders\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.spines['left'].set_linewidth(1.5)\n",
    "    ax.spines['bottom'].set_linewidth(1.5)\n",
    "\n",
    "# Add a title\n",
    "# fig.suptitle('Weighted Loss vs Probability of Prediction')\n",
    "\n",
    "# Add a grid\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.grid(True, linestyle='-', alpha=0.8)\n",
    "    # ax.grid(axis='y', linestyle=':', alpha=0.7)\n",
    "\n",
    "# Improve axis labels\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.set_xlabel('Probability of prediction')\n",
    "    ax.set_ylabel('Weight')\n",
    "\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.legend(loc='best', frameon=False)\n",
    "\n",
    "# Adjust aspect ratio\n",
    "fig.subplots_adjust(wspace=0.3)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"function.eps\", format='eps', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 折线图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.font_manager import FontProperties\n",
    "# 数据\n",
    "x = [i*1000 for i in range(1,5)]\n",
    "baseline_no_prompt_acc_mean_5 = [83.63999938964844,84.23999938964843,86.2,86.64000091552734]\n",
    "baseline_acc_mean_5 = [83.15999908447266,85.44000091552735,86.28000183105469,86.51999969482422]\n",
    "ensemble_acc_mean_5 = [85.68000183105468,87.6,87.16000061035156,87.96000061035156]\n",
    "baseline_no_prompt_f1_mean_5 = [79.65749969482422,80.31880798339844,82.51792449951172,82.95933532714844]\n",
    "baseline_f1_mean_5 = [79.35353546142578,81.92408599853516,82.49300231933594,82.68881683349609]\n",
    "ensemble_f1_mean_5 = [81.37353973388672,83.64531707763672,82.87378692626953,84.07548522949219]\n",
    "\n",
    "\n",
    "# 设置图形参数\n",
    "# font = FontProperties(fname='/home/jjwang/.fonts/cmunrm.ttf')\n",
    "# plt.rcParams['font.family'] = font.get_name()\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "# plt.rcParams['font.serif'] = ['CMU Serif'] + plt.rcParams['font.serif']\n",
    "# plt.rcParams['font.serif'] = 'Times New Roman'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']\n",
    "print(plt.rcParams['font.serif'])\n",
    "# plt.rc('font', family='CMU Serif')\n",
    "plt.rcParams['font.size'] = 8\n",
    "plt.rcParams['axes.linewidth'] = 1.5\n",
    "plt.rcParams['xtick.major.width'] = 0.5\n",
    "plt.rcParams['ytick.major.width'] = 0.5\n",
    "plt.rcParams['lines.linewidth'] = 1.25\n",
    "\n",
    "\n",
    "# Use a pastel color scheme\n",
    "colors = sns.color_palette(\"Spectral\", 2)\n",
    "colors = sns.color_palette(\"husl\", 2)\n",
    "colors2 = sns.color_palette(\"Greens\", 2)\n",
    "colors = [colors[1], colors2[1]]\n",
    "colors = ['#37B971', '#ADD71B']\n",
    "# colors = sns.dark_palette(\"Greens\", 4, reverse=True)\n",
    "# colors = sns.dark_palette(\"deep\", 4, reverse=True)\n",
    "\n",
    "\n",
    "# 创建图形\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(9, 2.5), dpi=300)\n",
    "\n",
    "# 绘制Accuracy子图\n",
    "# ax1.plot(x, baseline_no_prompt_acc_mean_5, 'v-', color='tab:red', label='Baseline_no_prompt')\n",
    "# ax1.plot(x, baseline_acc_mean_5, 'o-', color='tab:blue', label='Baseline')\n",
    "# ax1.plot(x, ensemble_acc_mean_5, 'D-', color='tab:green', label='With describe')\n",
    "ax1.axhline(y=81.875, linestyle='--', color='red', label='ChatGPT Zeroshot')\n",
    "ax1.plot(x, baseline_acc_mean_5, 'o-', color=colors[0], label='Baseline')\n",
    "ax1.plot(x, ensemble_acc_mean_5, 'D-', color=colors[1], label='With Distinctions')\n",
    "ax1.set_xlabel('Number of Training Examples')\n",
    "ax1.set_ylabel('Accuracy (%)')\n",
    "# ax1.legend(loc='lower right')\n",
    "# ax1.grid(axis='y', linestyle='-', alpha=0.8)\n",
    "# 绘制F1-Score子图\n",
    "# ax2.plot(x, baseline_no_prompt_f1_mean_5, 'v-', color='tab:red', label='Baseline_no_prompt')\n",
    "ax2.axhline(y=75.65480188045667, linestyle='--', color='red', label='ChatGPT Zeroshot')\n",
    "ax2.plot(x, baseline_f1_mean_5, 'o-', color=colors[0], label='Baseline')\n",
    "ax2.plot(x, ensemble_f1_mean_5, 'D-', color=colors[1], label='With Distinctions')\n",
    "ax2.set_xlabel('Number of Training Examples')\n",
    "ax2.set_ylabel('F1-Score (%)')\n",
    "# ax2.legend(loc='lower right')\n",
    "# ax2.grid(axis='y', linestyle='-', alpha=0.8)\n",
    "# 调整子图间距和布局\n",
    "fig.subplots_adjust(wspace=0.3)\n",
    "\n",
    "for ax in (ax1, ax2):\n",
    "    ax.grid(axis='y', linestyle='-', alpha=0.8)\n",
    "    ax.legend(loc='lower right', bbox_to_anchor=(1, 0.05))\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    # ax.spines['bottom'].set_linewidth(2)\n",
    "    # ax.spines['left'].set_linewidth(2)\n",
    "# 保存图形\n",
    "# fig.savefig('subplots.pdf', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"gpt.eps\", format='eps', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rouge-L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional.text.rouge import rouge_score\n",
    "\n",
    "\n",
    "all_rougeL = []\n",
    "neg_rougel = []\n",
    "pos_rougel = []\n",
    "lines = [\n",
    "    '头发多适合剪短发吗   头发少适合剪短吗    0', \n",
    "    '今天北京下雨了吗   今天北京会下雨吗    0', \n",
    "    '刚出生的小野鸡怎么养   刚抓来的野鸡怎么养    0', \n",
    "    '刚出生的小野鸡怎么养\t刚抓来的野鸡怎么养殖  0', \n",
    "    '温州有什么好玩的地方？ 温州什么地方最好玩  0',\n",
    "    '今晚吃什么    篮球好玩吗   0', \n",
    "    '这姑娘漂亮不   我姑娘漂亮吧    0', \n",
    "    '意大利面怎么做   意大利面怎么煮的快  0', \n",
    "    '瞻仰的瞻是什么意思   瞻仰的仰是什么意思？！ 0', \n",
    "    '仰望的仰是什么意思   瞻仰的仰是什么意思 0', \n",
    "    '赵县梨花什么时候开 赵县梨花节什么时候啊？ 0', \n",
    "    '晚安日语怎么写？ 晚安用日语怎么说 0',\n",
    "    ]\n",
    "\n",
    "\n",
    "for line in lines:\n",
    "    s1, s2, label = line.split()\n",
    "    lable = int(label)\n",
    "    rougeL = rouge_score(s1, s2, rouge_keys='rougeL', tokenizer=list, normalizer=lambda x: x)['rougeL_fmeasure'].item()\n",
    "    all_rougeL.append(rougeL)\n",
    "    if lable==1:\n",
    "        pos_rougel.append(rougeL)\n",
    "    else:\n",
    "        neg_rougel.append(rougeL)\n",
    "    print(f\"{s1}\\t{s2}\\t{rougeL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "\n",
    "lines = [\n",
    "    '头发多适合剪短发吗   头发少适合剪短吗    0', \n",
    "    '今天北京下雨了吗   今天北京会下雨吗    0', \n",
    "    '刚出生的小野鸡怎么养   刚抓来的野鸡怎么养    0', \n",
    "    '刚出生的小野鸡怎么养\t刚抓来的野鸡怎么养殖  0', \n",
    "    '温州有什么好玩的地方？ 温州什么地方最好玩  0',\n",
    "    '今晚吃什么    篮球好玩吗   0', \n",
    "    '这姑娘漂亮不   我姑娘漂亮吧    0', \n",
    "    '意大利面怎么做   意大利面怎么煮的快  0', \n",
    "    '瞻仰的瞻是什么意思   瞻仰的仰是什么意思？！ 0', \n",
    "    '仰望的仰是什么意思   瞻仰的仰是什么意思 0', \n",
    "    '赵县梨花什么时候开 赵县梨花节什么时候啊？ 0', \n",
    "    '晚安日语怎么写？ 晚安用日语怎么说 0',\n",
    "    ]\n",
    "\n",
    "\n",
    "all_lr = []\n",
    "neg_lr = []\n",
    "pos_lr = []\n",
    "\n",
    "for line in lines:\n",
    "    s1, s2, label = line.split()\n",
    "    lable = int(label)\n",
    "    lr = Levenshtein.ratio(s1, s2)\n",
    "    all_lr.append(lr)\n",
    "    if lable==1:\n",
    "        pos_lr.append(lr)\n",
    "    else:\n",
    "        neg_lr.append(lr)\n",
    "    print(f\"{s1}\\t{s2}\\t{lr}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = 'qnli'\n",
    "dataset = load_dataset(\"glue\", dataset_name)\n",
    "# you can use any of the following config names as a second argument:\n",
    "\"ax\", \"cola\", \"mnli\", \"mnli_matched\", \n",
    "\"mnli_mismatched\", \"mrpc\", \"qnli\", \"qqp\", \n",
    "\"rte\", \"sst2\", \"stsb\", \"wnli\"\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "for key in dataset['train'].features.keys():\n",
    "    pprint(f\"{key}: {dataset['train'][key][:10]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import os\n",
    "from tqdm.auto import trange\n",
    "dataset_name = dataset_name.upper()\n",
    "splits = dataset.keys()\n",
    "# split = 'validation'\n",
    "for split in splits:\n",
    "    output_path = f\"data/{dataset_name}/{split}\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    with jsonlines.open(os.path.join(output_path, 'all.jsonl'), 'w') as jlWriter:\n",
    "        objs = []\n",
    "        keys = list(dataset[split].features.keys())\n",
    "        data_dict = {key:dataset[split][key] for key in keys}\n",
    "        for i in trange(dataset[split].num_rows):\n",
    "            objs.append({key:data_dict[key][i] for key in keys})\n",
    "        jlWriter.write_all(objs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
