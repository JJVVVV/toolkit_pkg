{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.zeros((3,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': [[1, 2, 3], [3, 4, 5]]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'a': [[1,2,3], [3,4,5]]}\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': [[1, 2], [3, 4, 5]]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['a'][0] = [1,2]\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "my_dict = {'a': 1, 'b': 2, 'c': 3}\n",
    "value = next(iter(my_dict.values()))\n",
    "print(value)  # 输出 1 或 2 或 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [3, 32, 15, 7]\n",
    "num_tokens_to_remove = 20\n",
    "while num_tokens_to_remove:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9324193000793457\n",
      "[264, 265]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "num = 50000\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for _ in range(num):\n",
    "    lengths = [312, 267]\n",
    "    num_tokens_to_remove = 50\n",
    "    lengths_waiting_to_trunc = [lengths[i] for i in [0,1]]\n",
    "    for _ in range(num_tokens_to_remove):\n",
    "        index_max = lengths_waiting_to_trunc.index(max(lengths_waiting_to_trunc))\n",
    "        lengths_waiting_to_trunc[index_max] -= 1\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)\n",
    "print(lengths_waiting_to_trunc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.97226357460022\n",
      "[310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 310, 267, 311, 267, 311, 267, 311, 267, 311, 267, 311, 267, 311, 267, 311, 267, 311, 267, 311, 267, 311, 267]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "def argmax(l: list):\n",
    "    max_value_idx = 0\n",
    "    max_value = l[0]\n",
    "    idx = 1\n",
    "    while idx<len(l):\n",
    "        if max_value<l[idx]:\n",
    "            max_value, max_value_idx = l[idx], idx\n",
    "        idx += 1\n",
    "    return max_value_idx\n",
    "\n",
    "\n",
    "lengths = [312, 267]\n",
    "x = 50\n",
    "start = time.time()\n",
    "for _ in range(num):\n",
    "    lengths = [312, 267]*30\n",
    "    for _ in range(x):\n",
    "        max_index = argmax(lengths)\n",
    "        lengths[max_index] -= 1\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)\n",
    "print(lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.818470001220703\n",
      "[295 267 295 267 296 267]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "lengths = [312, 267]\n",
    "x = 50\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(num):\n",
    "    lengths = [312, 267]*3\n",
    "    for _ in range(x):\n",
    "        lengths = np.array(lengths)\n",
    "        max_index = np.argmax(lengths)\n",
    "        lengths[max_index] -= 1\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)\n",
    "print(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 2, 3, 6, 7, 8], 'ttt': [5, 4, 3, 6, 7, 8]}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def __truncate(tokenized_dict: dict[str, list[list]], waiting_to_trunc_idxs: list, num_tokens_to_remove: int) -> None:\n",
    "    lengths = [len(value_part) for value_part in tokenized_dict[\"input_ids\"]]\n",
    "    lengths_waiting_to_trunc = [lengths[i] for i in waiting_to_trunc_idxs]\n",
    "    for _ in range(num_tokens_to_remove):\n",
    "        index_max = lengths_waiting_to_trunc.index(max(lengths_waiting_to_trunc))\n",
    "        lengths_waiting_to_trunc[index_max] -= 1\n",
    "    print(lengths_waiting_to_trunc)\n",
    "    for i, j in zip(waiting_to_trunc_idxs, range(len(lengths_waiting_to_trunc))):\n",
    "        lengths[i] = lengths_waiting_to_trunc[j]\n",
    "    for value in tokenized_dict.values():\n",
    "        for i in waiting_to_trunc_idxs:\n",
    "            value[i] = value[i][: lengths[i]]\n",
    "\n",
    "cur_dict = {'input_ids':[[1,2,3,4,5], [6,7,8,9,0]], 'ttt': [[5,4,3,2,1], [6,7,8,9,0]]}\n",
    "__truncate(cur_dict, [0,1], 4)\n",
    "cur_dict\n",
    "cur_dict = {key: sum(value, []) for key, value in cur_dict.items()}\n",
    "cur_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [7592, 1010, 2129, 2024, 2017, 1012, 5292, 3270, 23644, 2050], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer('hello, how are you. hahahha', add_special_tokens=False)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = dict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [7592, 1010, 2129, 2024, 2017, 1012, 5292, 3270, 23644, 2050],\n",
       " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inputs)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.pad(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-26 10:46:29,667 - \u001b[33m WARNING > toolkit.nlp.dataset: model input include 'token_type_ids'. There is a bug causing all the token_type_ids to be zeros\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8bf2e9ceb414de99d87d3ccf5c7e536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenize input texts:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from toolkit.nlp.dataset import MyDataset\n",
    "from transformers import AutoTokenizer\n",
    "import jsonlines\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "def myfun(file_path, tokenizer, model_type, **kargs):\n",
    "    special_tokens_map = tokenizer.special_tokens_map\n",
    "    BOS = special_tokens_map[\"bos_token\"] if \"bos_token\" in special_tokens_map.keys() else None\n",
    "    EOS = special_tokens_map[\"eos_token\"] if \"eos_token\" in special_tokens_map.keys() else None\n",
    "    SEP = special_tokens_map[\"sep_token\"] if \"sep_token\" in special_tokens_map.keys() else None\n",
    "    MASK = special_tokens_map[\"mask_token\"] if \"mask_token\" in special_tokens_map.keys() else None\n",
    "    CLS = special_tokens_map[\"cls_token\"] if \"cls_token\" in special_tokens_map.keys() else None\n",
    "    sep_num = 1\n",
    "\n",
    "    with jsonlines.open(file_path, \"r\") as jlReader:\n",
    "        dict_objs = list(jlReader)\n",
    "        if isinstance(dict_objs[0], str):\n",
    "            dict_objs = dict_objs[1:]\n",
    "\n",
    "    input_texts = []\n",
    "    labels = []\n",
    "    for dict_obj in dict_objs:\n",
    "        # input_texts.append(f\"{BOS}{dict_obj['question1']}{SEP*sep_num}{dict_obj['question2']}{EOS}\")\n",
    "        input_texts.append(((False, CLS), (True, dict_obj[\"question1\"]), (False, SEP), (True, dict_obj[\"question2\"]), (False, SEP)))\n",
    "        labels.append([dict_obj[\"label\"]])\n",
    "        # labels.append(((False, CLS), (True, dict_obj[\"question1\"])))\n",
    "        # labels.append(dict_obj[\"question1\"])\n",
    "    return input_texts, labels\n",
    "\n",
    "\n",
    "dataset = MyDataset(\n",
    "    \"test/data.jsonl\",\n",
    "    \"bert-base-uncased\",\n",
    "    tokenizer,\n",
    "    input_text_format_func=myfun,\n",
    "    padding_side=\"left\",\n",
    "    max_length_input=256,\n",
    "    max_length_label=256,\n",
    "    is_train=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'token_type_ids', 'attention_mask']"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.model_input_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_input': {'input_ids': tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "         101, 2339, 2024, 3060, 1011, 4841, 2061, 3376, 1029,  102, 2339, 2024,\n",
      "        6696, 2015, 2061, 3376, 1029,  102]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1])}, 'first_not_pad_index_input': tensor(36), 'labels': tensor([0], dtype=torch.int32)}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "1\n",
      "[CLS] i want to pursue phd in computer science about social network, what is the open problem in social networks? [SEP] i handle social media for a non - profit. should i start going to social media networking events? are there any good ones in the bay area? [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(dataset.max_length_inputs)\n",
    "print(dataset.max_length_labels)\n",
    "print(tokenizer.decode(dataset[1]['model_input']['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 54])\n",
      "torch.Size([4, 39])\n",
      "torch.Size([4, 46])\n",
      "torch.Size([4, 33])\n",
      "torch.Size([4, 36])\n",
      "torch.Size([4, 48])\n",
      "torch.Size([4, 35])\n",
      "torch.Size([4, 50])\n",
      "{'labels': tensor([[0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0]], dtype=torch.int32), 'input_ids': tensor([[    0,     0,     0,     0,     0,     0,     0,   101,  2054,  2024,\n",
      "          1996,  5918,  2000,  2468,  2343,  1999,  1996,  2142,  2163,  1998,\n",
      "          2129,  2024,  1996,  5918,  2367,  1999, 11959,  1029,   102,  2054,\n",
      "          2024,  1996,  5918,  2000,  2468,  2343,  1999,  1996,  2142,  2163,\n",
      "          1998,  2129,  2024,  1996,  5918,  2367,  1999,  2605,  1029,   102],\n",
      "        [  101,  1000,  2040,  2003,  1996,  4205,  1996,  4581,  2000, 12373,\n",
      "          1011, 17691,  1005,  1055,  1000,  1000,  4205,  1005,  1055,  2299,\n",
      "          1000,  1000,  2024,  2517,  2055,  1029,  1000,   102,  1000,  2029,\n",
      "         12373,  1011, 17691,  2316,  2266,  2626,  1996,  4581,  2000,  1000,\n",
      "          1000,  4205,  1005,  1055,  2299,  1000,  1000,  1029,  1000,   102],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,   101,  2054,  2808,  2323,  1045,\n",
      "          3191,  2004,  2019, 22344, 10670,  1029,   102,  2054,  2024,  1996,\n",
      "          2327,  2808,  2019, 22344,  9458, 10670,  2323,  3191,  1029,   102],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,   101,  2339,  2515,  1037,\n",
      "         10098, 18669, 20553,  2102,  2191,  1037,  2204,  9004,  1029,   102,\n",
      "          2064, 18669, 20553,  3215,  2022,  2204, 18551,  1029,  2054,  2024,\n",
      "          2070,  3971,  2000,  2202,  2204,  2729,  1997,  2068,  1029,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, default_collate\n",
    "\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=4, shuffle=False, \n",
    "                        collate_fn=dataset.collate_fn_padding_right if dataset.padding_side=='right' else dataset.collate_fn_padding_left, \n",
    "                        pin_memory=True)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch['input_ids'].shape)\n",
    "    # print(batch['labels'].shape)\n",
    "    # print(batch['input_ids'].is_pinned())\n",
    "    # print(batch)\n",
    "    # break\n",
    "\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': tensor([[1, 2, 3],\n",
       "         [1, 2, 3]]),\n",
       " 'b': tensor([[4, 5, 6],\n",
       "         [4, 5, 6]]),\n",
       " 'c': ['hh', 'hh']}"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, default_collate\n",
    "\n",
    "default_collate([{'a': torch.tensor([1,2,3]), 'b': torch.tensor([4,5,6]), 'c':'h' 'h'}, {'a': torch.tensor([1,2,3]), 'b': torch.tensor([4,5,6]), 'c':'h' 'h'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
