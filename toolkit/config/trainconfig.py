from collections import OrderedDict
from pathlib import Path
from typing import Self

from .config_base import ConfigBase, logger

from ..metric.metricdict import MetricDict
from .config_base import ConfigBase

# SILENCE = True
CONFIG_NAME = "train_config.json"


class TrainConfig(ConfigBase):
    def __init__(
        self,
        seed: int = 0,
        gpu: bool = True,
        problem_type: str | None = None,
        dataset_name: str = "",
        train_file_path: Path | str | None = None,
        val_file_path: Path | str | None = None,
        test_file_path: Path | str | None = None,
        model_type: str = "",
        model_name: str = "",
        model_dir: str | None = None,
        metric: str = "Loss",
        epochs: int = 0,
        train_batch_size: int = 0,
        infer_batch_size: int = 0,
        opt_type: str | None = None,
        opt_lr: float = 1e-4,
        opt_betas1: float = 0.9,
        opt_betas2: float = 0.999,
        opt_eps: float = 1e-8,
        opt_weight_decay: float = 0.01,
        sch_type: str | None = None,
        sch_warmup_min_lr: float = 0,
        sch_warmup_max_lr: float = 0,
        sch_warmup_num_steps: int = -1,
        sch_warmup_ratio_steps: float = -1,
        sch_total_num_steps: int = -1,
        save_dir: Path | str | None = None,
        run_dir: Path | str | None = None,
        early_stop: bool = False,
        patience: int = -1,
        continue_train_more_patience: bool = False,
        eval_every_half_epoch: bool = False,
        eval_step: int = 0,
        save_all_ckpts: bool = False,
        max_optimal_ckpt_num: int = 1,
        cache_dataset: bool = False,
        gradient_accumulation_steps: int = 1,
        gradient_clipping: float = 1.0,
        parallel_mode: str | None = None,
        ddp_timeout: int = 1800,
        fp16: bool = False,
        dashboard: str | None = None,
        shuffle: bool | None = None,
        logging_steps: int = -1,
        **kwargs,
    ):
        super().__init__(**kwargs)
        # initialize
        self.seed = seed
        self.gpu = gpu

        # attributes related to the data
        self.problem_type = problem_type
        allowed_problem_types = ("regression", "single_label_classification", "multi_label_classification")
        if self.problem_type is not None and self.problem_type not in allowed_problem_types:
            raise ValueError(
                f"The config parameter `problem_type` was not understood: received {self.problem_type} "
                "but only 'regression', 'single_label_classification' and 'multi_label_classification' are valid."
            )
        self.dataset_name = dataset_name
        self.train_file_path = Path(train_file_path) if train_file_path is not None else None
        self.val_file_path = Path(val_file_path) if val_file_path is not None else None
        self.test_file_path = Path(test_file_path) if test_file_path is not None else None
        self.check_data_file()

        # attributes related to the model
        self.model_type = model_type
        self.model_name = model_name
        self.model_dir = model_dir

        # attributes related to the metric
        self.metric = metric
        if self.metric not in MetricDict.support_metrics():
            raise ValueError(
                f"The config parameter `metric` was not understood: received `{self.metric}` "
                f"but only {[key for key in  MetricDict.support_metrics()]} are valid."
            )

        # attributes related to training steps
        self.epochs = epochs
        self.train_batch_size = train_batch_size
        self.infer_batch_size = infer_batch_size if infer_batch_size != 0 else train_batch_size

        # optimizer
        self.opt_type = opt_type
        self.opt_lr = opt_lr
        self.opt_betas1 = opt_betas1
        self.opt_betas2 = opt_betas2
        self.opt_eps = opt_eps
        self.opt_weight_decay = opt_weight_decay

        # scheduler
        self.sch_type = sch_type
        self.sch_warmup_min_lr = sch_warmup_min_lr
        self.sch_warmup_max_lr = sch_warmup_max_lr if sch_warmup_max_lr != 0 else opt_lr
        self.sch_warmup_num_steps = sch_warmup_num_steps
        self.sch_warmup_ratio_steps = sch_warmup_ratio_steps
        self.sch_total_num_steps = sch_total_num_steps

        # outputs dir
        self.save_dir = Path(save_dir) if save_dir is not None else None
        self.run_dir = Path(run_dir) if run_dir is not None else None

        # during training
        self.early_stop = early_stop
        if self.early_stop:
            self.patience = patience
            self.continue_train_more_patience = continue_train_more_patience
        self.eval_every_half_epoch = eval_every_half_epoch
        self.eval_step = eval_step
        self.save_all_ckpts = save_all_ckpts
        self.gradient_clipping = gradient_clipping
        self.max_optimal_ckpt_num = max_optimal_ckpt_num

        # Optimization about load and memory
        self.cache_dataset = cache_dataset
        self.gradient_accumulation_steps = gradient_accumulation_steps
        assert parallel_mode in [None, "DDP", "deepspeed"], (
            f"[parallel_mode] Only `DDP` and `deepspeed` are supported, but got `{parallel_mode}`.\n"
            "if you do not need parallel, plase set it to `None`."
        )
        self.parallel_mode = parallel_mode
        self.fp16 = fp16

        # 自动计算的一些值
        self.total_steps_num = -1
        self.steps_per_epoch = -1
        
        # 杂项
        assert dashboard in ["wandb", "tensorboard", None], (
            f"Only `wandb` and `tensorboard` dashboards are supported, but got `{dashboard}`.\n"
            "if you do not need dashboard, plase set it to `None`."
        )
        self.dashboard = dashboard
        self.shuffle = shuffle
        self.ddp_timeout = ddp_timeout
        self.logging_steps = logging_steps
        # self.warning_default()

    def save(self, save_directory: Path | str, json_file_name=CONFIG_NAME, silence=True, **kwargs):
        if not silence:
            logger.debug(f"💾 Saving training configuration ...")
        super().save(save_directory, json_file_name, silence, **kwargs)

    @classmethod
    def load(cls, load_dir_or_path: Path | str, json_file_name=CONFIG_NAME, silence=True, **kwargs) -> Self:
        if not silence:
            logger.debug(f"💾 Loading training configuration ...")
        return super().load(load_dir_or_path, json_file_name, silence, **kwargs)

    def check_data_file(self):
        """
        Check whether the data files exist.
        """
        if self.train_file_path is not None:
            assert self.train_file_path.exists(), f"Training file: {self.train_file_path} dose not exists"
        if self.val_file_path is not None:
            assert self.val_file_path.exists(), f"Validation file: {self.val_file_path} dose not exists"
        if self.test_file_path is not None:
            assert self.test_file_path.exists(), f"Test file: {self.test_file_path} dose not exists"

    # 未使用
    def warning_default(self):
        default = self.__class__()
        attri_to_check = ("metric", "seed", "epochs", "batch_size", "learning_rate", "fp16")
        for attri in attri_to_check:
            if getattr(self, attri) != getattr(default, attri):
                logger.warning(f"`{attri}` is not specified, default value: `{getattr(default, attri)}`")

    def set_deepspeed(self, deepspeed_config: OrderedDict):
        """
        fill `auto` field of deepspeed config with trainer config
        """
        if (fp16 := deepspeed_config.get("fp16", None)) is not None:
            if fp16["enabled"] == "auto":
                fp16["enabled"] = self.fp16
        if (bf16 := deepspeed_config.get("bf16", None)) is not None:
            if bf16["enabled"] == "auto":
                bf16["enabled"] = self.bf16

        if (opt := deepspeed_config.get("optimizer", None)) is not None:
            if opt["type"] == "auto":
                opt["type"] = self.opt_type
            for key, value in opt["params"].items():
                if value == "auto":
                    if key != "betas":
                        opt["params"][key] = getattr(self, f"opt_{key}")
                    else:
                        opt["params"][key] = [self.opt_betas1, self.opt_betas2]
        if (sch := deepspeed_config.get("scheduler", None)) is not None:
            if sch["type"] == "auto":
                sch["type"] = self.sch_type
            for key, value in sch["params"].items():
                if value == "auto":
                    sch["params"][key] = getattr(self, f"sch_{key}")

        keys = ["gradient_accumulation_steps", "gradient_clipping", "train_batch_size"]
        for key in keys:
            if (value := deepspeed_config.get(key, None)) is not None:
                deepspeed_config[key] = getattr(self, key) if value == "auto" else value
